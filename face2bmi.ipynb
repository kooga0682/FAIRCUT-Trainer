{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "face2bmi.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kooga0682/FAIRCUT-Trainer/blob/master/face2bmi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69vuOD3VGETV",
        "colab_type": "code",
        "outputId": "9c69a09a-81ea-46ab-fb1f-afc2e93619ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaLkSinirITP",
        "colab_type": "code",
        "outputId": "e43a3d4d-387c-43a0-ce8a-7ea0b07ae313",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd /content/drive/My Drive/Make/face2bmi-keras/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Make/face2bmi-keras\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "833KRoR5GtXf",
        "colab_type": "code",
        "outputId": "31ffb153-8eed-4884-d288-ebd43bbaa3da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "# -*- coding: utf-8-unix -*-\n",
        "\n",
        "import os, sys\n",
        "sys.path.append(\"/face2bmi-keras/keras-vggface\")\n",
        "\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import h5py\n",
        "import re\n",
        "\n",
        "# Keras: Tensorflow backend\n",
        "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
        "from keras.utils import np_utils\n",
        "\n",
        "\n",
        "def numericalSort(value):\n",
        "    numbers = re.compile(r'(\\d+)')\n",
        "    parts = numbers.split(value)\n",
        "    parts[1::2] = map(int, parts[1::2])\n",
        "    return parts\n",
        "\n",
        "def flush_progress_bar(barname, rate):\n",
        "    number = int(rate/0.05)+1\n",
        "    bar = ('#' * number).ljust(20, '-')\n",
        "    \n",
        "    sys.stdout.write(f\"\\r{barname} : [{bar}] {rate*100:.2f}%\")\n",
        "\n",
        "class BMIDataset:\n",
        "    def __init__(self):\n",
        "        self.image_shape = (224, 224, 3)\n",
        "        self.dataset_store_file_path = \"./stored_bmi_dataset/bmi_dataset.h5\"\n",
        "        \n",
        "    def load_data(self, image_dir_path):\n",
        "        if os.path.exists(self.dataset_store_file_path):\n",
        "            print(\"Stored dataset found!\\nLoading...\")\n",
        "            \n",
        "            with h5py.File(self.dataset_store_file_path, 'r') as loaded_dataset:\n",
        "                x_train = loaded_dataset[\"x_train\"].value\n",
        "                t_train = loaded_dataset[\"t_train\"].value\n",
        "                x_test = loaded_dataset[\"x_test\"].value\n",
        "                t_test = loaded_dataset[\"t_test\"].value\n",
        "\n",
        "\n",
        "        else:\n",
        "            print(\"Stored dataset not found.\\nMaking...\")\n",
        "            x = []\n",
        "            t = []\n",
        "        \n",
        "            files = sorted(glob.glob(image_dir_path + \"/*.bmp\"), key=numericalSort)\n",
        "            \n",
        "            n = len(files)\n",
        "            i = 0\n",
        "            \n",
        "            # Get image and bmi value from original dataset\n",
        "            for imgfile in files:\n",
        "                img = load_img(imgfile, target_size=(224,224) ) #(self.image_shape[:2]))\n",
        "                img_array = img_to_array(img)\n",
        "                filename = os.path.basename(imgfile)\n",
        "                #bmi = self.filename2bmi(filename)\n",
        "                bmiLevel = self.filename2bmiLevel(filename)\n",
        "                \n",
        "                flush_progress_bar(\"Loading image\", i/n)\n",
        "                \n",
        "                x.append(img_array)\n",
        "                t.append(bmiLevel)\n",
        "        \n",
        "                i += 1\n",
        "        \n",
        "            x = np.array(x)\n",
        "            x = x.astype(\"float32\") / 255.0\n",
        "            t = np.array(t)\n",
        "            t = t.astype(\"int32\")\n",
        "            t_categorical = np_utils.to_categorical(t, 9)\n",
        "\n",
        "            x_train = x[:3368]\n",
        "            x_test = x[3368:]\n",
        "            t_train = t_categorical[:3368]\n",
        "            t_test = t_categorical[3368:]\n",
        "            \n",
        "            # Store dataset to file\n",
        "            if not os.path.exists(os.path.dirname(self.dataset_store_file_path)):\n",
        "                os.mkdir(os.path.dirname(self.dataset_store_file_path))\n",
        "            \n",
        "            with h5py.File(self.dataset_store_file_path, \"w\") as store_dataset:\n",
        "                store_dataset.create_dataset(\"x_train\", data=x_train)\n",
        "                store_dataset.create_dataset(\"t_train\", data=t_train)\n",
        "                store_dataset.create_dataset(\"x_test\", data=x_test)\n",
        "                store_dataset.create_dataset(\"t_test\", data=t_test)\n",
        "                store_dataset.flush()\n",
        "    \n",
        "        return((x_train, t_train), (x_test, t_test))\n",
        "\n",
        "    def filename2bmiLevel(self, filename):    \n",
        "        # Get index\n",
        "        tempstr = filename.replace(\".bmp\", \"\")\n",
        "        tempstrs = tempstr.split(\"_\")\n",
        "        index = int(tempstrs[1])\n",
        "\n",
        "        # Get BMI from csv file by index\n",
        "        df = pd.read_csv(\"./DataFromMIT-CSAIL/data.csv\")\n",
        "        \n",
        "        bmi = df[\"bmi\"][index]\n",
        "\n",
        "        bmiLevel = 1\n",
        "\n",
        "        if bmi < 16.0:\n",
        "            bmiLevel = 8\n",
        "        elif bmi >= 16.0 and bmi < 17.0:\n",
        "            bmiLevel = 7\n",
        "        elif bmi >= 17.0 and bmi < 18.5:\n",
        "            bmiLevel = 6\n",
        "        elif bmi >= 18.5 and bmi < 25.0:\n",
        "            bmiLevel = 5\n",
        "        elif bmi >= 25.0 and bmi < 30.0:\n",
        "            bmiLevel = 4\n",
        "        elif bmi >= 30.0 and bmi < 35.0:\n",
        "            bmiLevel = 3\n",
        "        elif bmi >= 35.0 and bmi < 40.0:\n",
        "            bmiLevel = 2\n",
        "        elif bmi >= 40.0:\n",
        "            bmiLevel = 1\n",
        "        else:\n",
        "            bmiLevel = 5\n",
        "            \n",
        "        return bmiLevel\n",
        "    \n",
        "    \n",
        "    def filename2bmi(self, filename):    \n",
        "        # Get index\n",
        "        tempstr = filename.replace(\".bmp\", \"\")\n",
        "        tempstrs = tempstr.split(\"_\")\n",
        "        index = int(tempstrs[1])\n",
        "\n",
        "        # Get BMI from csv file by index\n",
        "        df = pd.read_csv(\"./DataFromMIT-CSAIL/data.csv\")\n",
        "        \n",
        "        bmi = df[\"bmi\"][index]\n",
        "        \n",
        "        return float(bmi)\n",
        "\n",
        "\n",
        "\n",
        "def main():\n",
        "    \n",
        "    # Load dataset\n",
        "    dataset = BMIDataset()\n",
        "    ( (x_train, t_train), (x_test, t_test) ) = dataset.load_data(\"./DataFromMIT-CSAIL/Images\")\n",
        "\n",
        "    print('\\n')\n",
        "    \n",
        "    print(t_test[:10])\n",
        "    \n",
        "    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stored dataset found!\n",
            "Loading...\n",
            "\n",
            "\n",
            "[[0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzGqiP8kGrbc",
        "colab_type": "code",
        "outputId": "b5f789e3-67fc-4471-d468-cb4882d0c0f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 958
        }
      },
      "source": [
        "# -*- coding: utf-8-unix -*-\n",
        "\n",
        "import os, sys\n",
        "sys.path.append(\"./keras-vggface\")\n",
        "\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "from keras.preprocessing.image import load_img, img_to_array, array_to_img\n",
        "from keras.engine import  Model\n",
        "from keras.layers import Flatten, Dense, Input, Dropout\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.optimizers import SGD\n",
        "from keras_vggface.vggface import VGGFace\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.models import load_model\n",
        "\n",
        "\n",
        "\n",
        "def network():\n",
        "    #custom parameters\n",
        "    nb_class = 9\n",
        "    hidden_dim = 512\n",
        "    learning_late = 0.01\n",
        "\n",
        "    vgg_model = VGGFace(include_top=False, input_shape=(224, 224, 3))\n",
        "    last_layer = vgg_model.get_layer('pool5').output\n",
        "    x = Flatten(name='flatten')(last_layer)\n",
        "    x = Dense(hidden_dim, activation='relu', name='fc6')(x)\n",
        "    x = Dense(hidden_dim, activation='relu', name='fc7')(x)\n",
        "    out = Dense(nb_class, activation='softmax', name='fc8')(x)\n",
        "    custom_vgg_model = Model(vgg_model.input, out)\n",
        "    vgg_model.summary()\n",
        "    optimizer = SGD(lr=learning_late)\n",
        "    custom_vgg_model.compile(loss=\"categorical_crossentropy\", optimizer=optimizer, metrics=[\"mae\"])\n",
        "\n",
        "    return custom_vgg_model\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Parameters\n",
        "    batch_size = 50\n",
        "    epochs = 400\n",
        "    verbose = 0 #1\n",
        "    validation_split = 0.2\n",
        "    \n",
        "    # Load dataset\n",
        "    dataset = BMIDataset()\n",
        "    (x_train, t_train), (x_test, t_test) = dataset.load_data(\"./Data/Images\")\n",
        "    input_shape = dataset.image_shape\n",
        "    \n",
        "    model = network()    \n",
        "    \n",
        "    \n",
        "    print(t_test)\n",
        "\n",
        "    \n",
        "    # Learn\n",
        "    model.fit(x_train, t_train,\n",
        "              batch_size=batch_size, epochs=epochs, callbacks=None, \n",
        "              verbose=verbose, validation_split=validation_split)\n",
        "    model.save('saved_model.h5')\n",
        "    \n",
        "    # # Eval\n",
        "    score = model.evaluate(x_test, t_test, verbose=verbose, batch_size=batch_size)\n",
        "    print(\"\\nTest loss:\", score[0])\n",
        "    print('Test mean abs error:', score[1])\n",
        "    \n",
        "    \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stored dataset found!\n",
            "Loading...\n",
            "Downloading data from https://github.com/rcmalli/keras-vggface/releases/download/v2.0/rcmalli_vggface_tf_notop_vgg16.h5\n",
            "58916864/58909280 [==============================] - 3s 0us/step\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv1_1 (Conv2D)             (None, 224, 224, 64)      1792      \n",
            "_________________________________________________________________\n",
            "conv1_2 (Conv2D)             (None, 224, 224, 64)      36928     \n",
            "_________________________________________________________________\n",
            "pool1 (MaxPooling2D)         (None, 112, 112, 64)      0         \n",
            "_________________________________________________________________\n",
            "conv2_1 (Conv2D)             (None, 112, 112, 128)     73856     \n",
            "_________________________________________________________________\n",
            "conv2_2 (Conv2D)             (None, 112, 112, 128)     147584    \n",
            "_________________________________________________________________\n",
            "pool2 (MaxPooling2D)         (None, 56, 56, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv3_1 (Conv2D)             (None, 56, 56, 256)       295168    \n",
            "_________________________________________________________________\n",
            "conv3_2 (Conv2D)             (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "conv3_3 (Conv2D)             (None, 56, 56, 256)       590080    \n",
            "_________________________________________________________________\n",
            "pool3 (MaxPooling2D)         (None, 28, 28, 256)       0         \n",
            "_________________________________________________________________\n",
            "conv4_1 (Conv2D)             (None, 28, 28, 512)       1180160   \n",
            "_________________________________________________________________\n",
            "conv4_2 (Conv2D)             (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "conv4_3 (Conv2D)             (None, 28, 28, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "pool4 (MaxPooling2D)         (None, 14, 14, 512)       0         \n",
            "_________________________________________________________________\n",
            "conv5_1 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "conv5_2 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "conv5_3 (Conv2D)             (None, 14, 14, 512)       2359808   \n",
            "_________________________________________________________________\n",
            "pool5 (MaxPooling2D)         (None, 7, 7, 512)         0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[[0. 0. 1. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 1. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6Ggv4MCT38K",
        "colab_type": "code",
        "outputId": "392b139b-3150-40b4-dc7d-9a03f334203b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 440
        }
      },
      "source": [
        "from keras.models import load_model\n",
        "\n",
        "batch_size = 50\n",
        "verbose = 1 #1\n",
        "validation_split = 0.2\n",
        "    \n",
        "# Load dataset\n",
        "dataset = BMIDataset()\n",
        "(x_train, t_train), (x_test, t_test) = dataset.load_data(\"./Data/Images\")\n",
        "input_shape = dataset.image_shape\n",
        "\n",
        "model = load_model('saved_model.h5')\n",
        "score = model.evaluate(x_test, t_test, verbose=verbose, batch_size=batch_size)\n",
        "print(\"\\nTest loss:\", score[0])\n",
        "print('Test mean abs error:', score[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stored dataset found!\n",
            "Loading...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0728 06:36:36.174724 140040920635264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0728 06:36:36.242967 140040920635264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0728 06:36:36.331175 140040920635264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0728 06:36:38.922859 140040920635264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0728 06:36:38.924287 140040920635264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "W0728 06:36:38.927668 140040920635264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "W0728 06:36:42.382308 140040920635264 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0728 06:36:42.520802 140040920635264 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "838/838 [==============================] - 18s 21ms/step\n",
            "\n",
            "Test loss: 5.587452828172852\n",
            "Test mean abs error: 0.14455433463708883\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}